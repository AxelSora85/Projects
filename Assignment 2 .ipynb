{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bd09c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of part A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as stats\n",
    "import nltk\n",
    "import string\n",
    "#import scipy as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45aeeb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            milked Unnamed: 1\n",
      "0          stories        NaN\n",
      "1           helped        NaN\n",
      "2           sorted        NaN\n",
      "3             lies        NaN\n",
      "4           selled        NaN\n",
      "5      adversities        NaN\n",
      "6     affectivites           \n",
      "7  affectabilities        NaN\n",
      "8          stabbed        NaN\n",
      "9            saved        NaN\n"
     ]
    }
   ],
   "source": [
    "sedData=pd.read_csv('TestData.csv')\n",
    "print(sedData)\n",
    "\n",
    "#tokenize string of text file\n",
    "\n",
    "#loop through your tokens and read stem function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfa35e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milk\n",
      "stor\n",
      "sort\n",
      "l\n"
     ]
    }
   ],
   "source": [
    "def case1(sedData):\n",
    "    if sedData.endswith(\"ed\"):\n",
    "        return sedData[:-2]\n",
    "    else:\n",
    "        return sedData\n",
    "\n",
    "\n",
    "def case2(sedData):\n",
    "    if sedData.endswith(\"ies\"):\n",
    "        return sedData[:-3]\n",
    "    else:\n",
    "        return sedData\n",
    "    \n",
    "words=\"milked\"\n",
    "megawords=\"stories\"\n",
    "superwords=\"sorted\"\n",
    "betterwords=\"lies\"\n",
    "print(case1(words))\n",
    "print(case2(megawords))\n",
    "print(case1(superwords))\n",
    "print(case2(betterwords))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "212873cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of part B\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stringText = ','.join(sedData)\n",
    "#print(stringText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09e52215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ItemID  Sentiment                                      SentimentText\n",
      "0           1          0                       is so sad for my APL frie...\n",
      "1           2          0                     I missed the New Moon trail...\n",
      "2           3          1                            omg its already 7:30 :O\n",
      "3           4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
      "4           5          0           i think mi bf is cheating on me!!!   ...\n",
      "...       ...        ...                                                ...\n",
      "99984   99996          0  @Cupcake  seems like a repeating problem   hop...\n",
      "99985   99997          1  @cupcake__ arrrr we both replied to each other...\n",
      "99986   99998          0                     @CuPcAkE_2120 ya i thought so \n",
      "99987   99999          1  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...\n",
      "99988  100000          1                    @cupcake_kayla haha yes you do \n",
      "\n",
      "[99989 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sedData=open(r'C:\\Users\\nford\\1_textmining\\Week5\\SentimentData.csv')\n",
    "#sedData=pd.read_csv('SentimentData', sep='|', encoding='latin-1')\n",
    "info = pd.read_csv('SentimentData.csv',encoding_errors='replace')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad08104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "insidetxt=info['SentimentText'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c26a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding stopwords\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "silly=['haha', 'lol', 'ha', 'ya', 'hey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38592cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number 2 \n",
    "checked_text=[]\n",
    "for s in info:\n",
    "    #remove @ symbols\n",
    "    s=re.sub('@\\S+',' ',s)\n",
    "    #remove punctuation\n",
    "    s=re.sub(r'[^\\w\\s]','',s)\n",
    "    #make lowercase\n",
    "    s=s.lower()\n",
    "    #remove stopwrods from text\n",
    "    s=' '.join([word for word in s.split()if word not in stopwords])\n",
    "    # Remove the non-lexical utterances (e.g. haha, lol, ha, ya, hey, etc)\n",
    "    s=' '.join([word for word in s.split() if word not in silly])\n",
    "    # Removed words with word lenght less then 2 char\n",
    "    s=' '.join([word for word in s.split() if len(word)>2])\n",
    "    checked_text.append(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de548006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 77,\n",
       " 'so': 136,\n",
       " 'sad': 128,\n",
       " 'for': 44,\n",
       " 'my': 101,\n",
       " 'the': 150,\n",
       " 'new': 104,\n",
       " 'its': 79,\n",
       " 'already': 4,\n",
       " 'im': 75,\n",
       " 've': 172,\n",
       " 'been': 19,\n",
       " 'at': 13,\n",
       " 'this': 157,\n",
       " 'was': 177,\n",
       " 'just': 80,\n",
       " 'get': 47,\n",
       " 'on': 115,\n",
       " 'think': 156,\n",
       " 'me': 96,\n",
       " 'or': 118,\n",
       " 'too': 165,\n",
       " 'much': 100,\n",
       " 'again': 2,\n",
       " 'work': 192,\n",
       " 'tomorrow': 163,\n",
       " 'tonight': 164,\n",
       " 'in': 76,\n",
       " 'today': 162,\n",
       " 'miss': 97,\n",
       " 'you': 198,\n",
       " 'how': 72,\n",
       " 'she': 132,\n",
       " 'about': 0,\n",
       " 'thanks': 147,\n",
       " 'to': 161,\n",
       " 'all': 3,\n",
       " 'up': 169,\n",
       " 'day': 32,\n",
       " 'has': 60,\n",
       " 'any': 10,\n",
       " 'more': 98,\n",
       " 'ok': 114,\n",
       " 'thats': 149,\n",
       " 'it': 78,\n",
       " 'lt': 92,\n",
       " 'way': 179,\n",
       " 'feel': 41,\n",
       " 'right': 127,\n",
       " 'now': 110,\n",
       " 'man': 94,\n",
       " 'can': 26,\n",
       " 'do': 35,\n",
       " 'twitter': 168,\n",
       " 'http': 73,\n",
       " 'gonna': 52,\n",
       " 'go': 50,\n",
       " 'some': 137,\n",
       " 'of': 111,\n",
       " 'off': 112,\n",
       " 'only': 117,\n",
       " 'well': 182,\n",
       " 'over': 121,\n",
       " 'happy': 59,\n",
       " 'very': 173,\n",
       " 're': 125,\n",
       " 'one': 116,\n",
       " 'who': 187,\n",
       " 'see': 131,\n",
       " 'no': 108,\n",
       " 'because': 18,\n",
       " 'awesome': 14,\n",
       " 'tweet': 166,\n",
       " 'and': 9,\n",
       " 'down': 39,\n",
       " 'had': 57,\n",
       " 'out': 120,\n",
       " 'something': 138,\n",
       " 'yeah': 195,\n",
       " 'that': 148,\n",
       " 'with': 191,\n",
       " 'him': 68,\n",
       " 'please': 123,\n",
       " 'tell': 144,\n",
       " 'be': 17,\n",
       " 'wish': 190,\n",
       " 'could': 31,\n",
       " 'here': 66,\n",
       " 'going': 51,\n",
       " 'like': 85,\n",
       " 'really': 126,\n",
       " 'want': 176,\n",
       " 'sleep': 135,\n",
       " 'but': 24,\n",
       " 'have': 62,\n",
       " 'an': 8,\n",
       " 'didn': 34,\n",
       " 'hate': 61,\n",
       " 'when': 185,\n",
       " 'guys': 56,\n",
       " 'not': 109,\n",
       " 'sure': 142,\n",
       " 'what': 184,\n",
       " 'are': 11,\n",
       " 'doing': 36,\n",
       " 'your': 199,\n",
       " 'bit': 23,\n",
       " 'morning': 99,\n",
       " 'need': 102,\n",
       " 'week': 181,\n",
       " 'dont': 38,\n",
       " 'cant': 27,\n",
       " 'great': 55,\n",
       " 'by': 25,\n",
       " 'amp': 7,\n",
       " 'last': 83,\n",
       " 'got': 54,\n",
       " 'from': 45,\n",
       " 'com': 28,\n",
       " 'even': 40,\n",
       " 'say': 130,\n",
       " 'show': 134,\n",
       " 'look': 90,\n",
       " 'time': 160,\n",
       " 'he': 63,\n",
       " 'should': 133,\n",
       " 'fun': 46,\n",
       " 'though': 158,\n",
       " 'will': 189,\n",
       " 'never': 103,\n",
       " 'love': 91,\n",
       " 'ur': 170,\n",
       " 'then': 152,\n",
       " 'always': 5,\n",
       " 'night': 107,\n",
       " 'find': 42,\n",
       " 'thought': 159,\n",
       " 'lol': 88,\n",
       " 'wanna': 175,\n",
       " 'home': 70,\n",
       " 'they': 154,\n",
       " 'make': 93,\n",
       " 'would': 193,\n",
       " 'if': 74,\n",
       " 'were': 183,\n",
       " 'sorry': 140,\n",
       " 'take': 143,\n",
       " 'come': 29,\n",
       " 'am': 6,\n",
       " 'watch': 178,\n",
       " 'hope': 71,\n",
       " 'll': 87,\n",
       " 'quot': 124,\n",
       " 'we': 180,\n",
       " 'let': 84,\n",
       " 'cool': 30,\n",
       " 'oh': 113,\n",
       " 'thank': 146,\n",
       " 'soon': 139,\n",
       " 'back': 15,\n",
       " 'next': 105,\n",
       " 'his': 69,\n",
       " 'there': 153,\n",
       " 'better': 22,\n",
       " 'getting': 48,\n",
       " 'her': 65,\n",
       " 'us': 171,\n",
       " 'still': 141,\n",
       " 'why': 188,\n",
       " 'where': 186,\n",
       " 'people': 122,\n",
       " 'bad': 16,\n",
       " 'good': 53,\n",
       " 'know': 82,\n",
       " 'don': 37,\n",
       " 'little': 86,\n",
       " 'best': 21,\n",
       " 'same': 129,\n",
       " 'ya': 194,\n",
       " 'wait': 174,\n",
       " 'twitpic': 167,\n",
       " 'maybe': 95,\n",
       " 'did': 33,\n",
       " 'haha': 58,\n",
       " 'thing': 155,\n",
       " 'than': 145,\n",
       " 'yes': 196,\n",
       " 'them': 151,\n",
       " 'as': 12,\n",
       " 'keep': 81,\n",
       " 'hey': 67,\n",
       " 'our': 119,\n",
       " 'being': 20,\n",
       " 'long': 89,\n",
       " 'nice': 106,\n",
       " 'after': 1,\n",
       " 'follow': 43,\n",
       " 'hear': 64,\n",
       " 'yet': 197,\n",
       " 'glad': 49}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = TfidfVectorizer(max_features = 200)\n",
    "#max features?\n",
    "vw=vocab.fit_transform(insidetxt)\n",
    "#assign to insidetxt with values\n",
    "\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64ea0929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itemid': 0, 'sentiment': 1, 'sentimenttext': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab cleanup with matrix \n",
    "vocab = TfidfVectorizer(max_features = 200)\n",
    "#checked text will be the cleanup text\n",
    "vw=vocab.fit_transform(checked_text)\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ee75db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the matrix \n",
    "vw.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9240d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have neither given nor received unauthorized aid in completing this work, nor have I presented someone else's work as my own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
